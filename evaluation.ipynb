{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to aspect_prediction_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# For pre-trained models eval:\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load data\n",
    "test_df = pd.read_csv(\"/Dataset/test_data/beauty_test.csv\")\n",
    "pred_df = pd.read_csv(\"./results/responses_beauty_w_keys.csv\")\n",
    "test_df[\"prediction\"] = pred_df[\"response\"]\n",
    "\n",
    "# Helper to safely parse dictionaries\n",
    "def safe_parse_dict(text, wrap_braces=False):\n",
    "    if pd.isna(text):\n",
    "        return {}\n",
    "    try:\n",
    "        if wrap_braces:\n",
    "            text = \"{\" + text + \"}\"\n",
    "        return ast.literal_eval(text)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "# Build comparison table\n",
    "comparison_data = []\n",
    "for i, row in test_df.iterrows():\n",
    "    gt = safe_parse_dict(row[\"aspects\"])\n",
    "    pred = safe_parse_dict(row[\"prediction\"], wrap_braces=True)\n",
    "    comparison_data.append({\n",
    "        \"product_id\": row[\"product_id\"],\n",
    "        \"video_title\": row[\"video_title\"],\n",
    "        \"ground_truth\": gt,\n",
    "        \"prediction\": pred\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.to_csv(\"./results/aspect_prediction_comparison.csv\", index=False)\n",
    "print(\"Exported to aspect_prediction_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute-Conditioned Evaluation\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load your comparison file\n",
    "df = pd.read_csv(\"./results/aspect_prediction_comparison.csv\")\n",
    "df_top10 = df[df[\"prediction\"].notna()]\n",
    "    \n",
    "# Helper to safely parse dictionaries\n",
    "def safe_parse(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "# Custom fuzzy match based on common substring rule (â‰¥ 50% of label length)\n",
    "def custom_fuzzy_match(label, pred):\n",
    "    label = str(label).lower()\n",
    "    pred = str(pred).lower()\n",
    "    match_length = len(os.path.commonprefix([label, pred]))\n",
    "    return match_length >= (len(label) / 2)\n",
    "\n",
    "# Compute F1 scores (overall and per attribute)\n",
    "def compute_fuzzy_f1_scores(df):\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    attr_stats = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        gt = safe_parse(row[\"ground_truth\"])\n",
    "        pred = safe_parse(row[\"prediction\"])\n",
    "\n",
    "        matched_keys = set()\n",
    "\n",
    "        for key, gt_val in gt.items():\n",
    "            if key in pred:\n",
    "                if custom_fuzzy_match(gt_val, pred[key]):\n",
    "                    total_tp += 1\n",
    "                    attr_stats[key][\"tp\"] += 1\n",
    "                else:\n",
    "                    total_fn += 1\n",
    "                    attr_stats[key][\"fn\"] += 1\n",
    "                matched_keys.add(key)\n",
    "            else:\n",
    "                total_fn += 1\n",
    "                attr_stats[key][\"fn\"] += 1\n",
    "\n",
    "        for key in pred:\n",
    "            if key not in gt:\n",
    "                total_fp += 1\n",
    "                attr_stats[key][\"fp\"] += 1\n",
    "            elif key not in matched_keys:\n",
    "                total_fp += 1\n",
    "                attr_stats[key][\"fp\"] += 1\n",
    "\n",
    "    # Overall scores\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    # Attribute-level scores\n",
    "    attr_f1_scores = {}\n",
    "    for attr, stats in attr_stats.items():\n",
    "        tp, fp, fn = stats[\"tp\"], stats[\"fp\"], stats[\"fn\"]\n",
    "        p = tp / (tp + fp) if tp + fp else 0\n",
    "        r = tp / (tp + fn) if tp + fn else 0\n",
    "        f1_attr = 2 * p * r / (p + r) if p + r else 0\n",
    "        attr_f1_scores[attr] = round(f1_attr, 4)\n",
    "\n",
    "    return round(precision, 4), round(recall, 4), round(f1, 4), attr_f1_scores\n",
    "\n",
    "# Run the function\n",
    "overall_precision, overall_recall, overall_f1, attribute_f1s = compute_fuzzy_f1_scores(df_top10)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nðŸ”¹ Overall F1:\")\n",
    "print(f\"Precision: {overall_precision}\")\n",
    "print(f\"Recall:    {overall_recall}\")\n",
    "print(f\"F1 Score:  {overall_f1}\\n\")\n",
    "\n",
    "print(\"ðŸ”¹ Attribute-level F1 scores (sorted by F1 descending):\")\n",
    "for attr, f1_score in sorted(attribute_f1s.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{attr}: {f1_score}\")\n",
    "# Count attribute frequency in ground truth\n",
    "attr_freq = defaultdict(int)\n",
    "for _, row in df_top10.iterrows():\n",
    "    gt = safe_parse(row[\"ground_truth\"])\n",
    "    for key in gt.keys():\n",
    "        attr_freq[key] += 1\n",
    "\n",
    "# Sort F1 scores by frequency descending\n",
    "sorted_attrs_by_freq = sorted(attribute_f1s.items(), key=lambda x: attr_freq.get(x[0], 0), reverse=True)\n",
    "\n",
    "print(\"ðŸ”¹ Attribute-level F1 scores (sorted by attribute frequency in ground truth):\")\n",
    "for attr, f1_score in sorted_attrs_by_freq:\n",
    "    print(f\"{attr} (count={attr_freq.get(attr, 0)}): {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized Evaluation\n",
    "import pandas as pd\n",
    "import difflib\n",
    "import ast\n",
    "import re\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"./results/aspect_prediction_comparison.csv\")\n",
    "df_pred = pd.read_csv(\"/Dataset/test_data/responses_beauty_generalized.csv\")\n",
    "\n",
    "# Filter out empty predictions\n",
    "df_top10 = df[df[\"prediction\"].notna()]\n",
    "\n",
    "# -------- Utility Functions --------\n",
    "\n",
    "def normalize(text):\n",
    "    return re.sub(r'\\s+', ' ', str(text).strip().lower())\n",
    "\n",
    "def fuzzy_match(s1, s2, threshold=50):\n",
    "    s1 = normalize(s1)\n",
    "    s2 = normalize(s2)\n",
    "    score = difflib.SequenceMatcher(None, s1, s2).ratio() * 100\n",
    "    return score >= threshold\n",
    "\n",
    "def match_key_fuzzy(pred_key, gt_keys, threshold=50):\n",
    "    pred_key_norm = normalize(pred_key)\n",
    "    for gt_key in gt_keys:\n",
    "        if difflib.SequenceMatcher(None, pred_key_norm, normalize(gt_key)).ratio() * 100 >= threshold:\n",
    "            return gt_key\n",
    "    return None\n",
    "\n",
    "def compute_fuzzy_f1(pred_dict, gt_dict, val_threshold=50, key_threshold=50):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    matched_gt_keys = set()\n",
    "    used_pred_keys = set()\n",
    "\n",
    "    for pred_key, pred_val in pred_dict.items():\n",
    "        matched_gt_key = match_key_fuzzy(pred_key, gt_dict.keys(), threshold=key_threshold)\n",
    "        if matched_gt_key and matched_gt_key not in matched_gt_keys:\n",
    "            if fuzzy_match(pred_val, gt_dict[matched_gt_key], threshold=val_threshold):\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "                FN += 1  # counts as both wrong and missing\n",
    "            matched_gt_keys.add(matched_gt_key)\n",
    "            used_pred_keys.add(pred_key)\n",
    "        else:\n",
    "            FP += 1\n",
    "\n",
    "    # Remaining ground truth keys are false negatives\n",
    "    for gt_key in gt_dict:\n",
    "        if gt_key not in matched_gt_keys:\n",
    "            FN += 1\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"TP\": TP,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# -------- Evaluation Loop --------\n",
    "\n",
    "gt = df_top10['ground_truth'].tolist()\n",
    "pred = df_pred['response'].tolist()\n",
    "\n",
    "TP_total = FP_total = FN_total = 0\n",
    "precision_total = recall_total = f1_total = 0\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    # Parse prediction\n",
    "    pred_pairs = re.findall(r\"'([^']+)': '([^']+)'\", str(pred[i]))\n",
    "    pred_dict = dict(pred_pairs)\n",
    "\n",
    "    # Parse ground truth\n",
    "    gt_dict = ast.literal_eval(gt[i])\n",
    "\n",
    "    # Compute per-instance scores\n",
    "    result = compute_fuzzy_f1(pred_dict, gt_dict, val_threshold=50, key_threshold=50)\n",
    "    TP_total += result['TP']\n",
    "    FP_total += result['FP']\n",
    "    FN_total += result['FN']\n",
    "    precision_total += result['precision']\n",
    "    recall_total += result['recall']\n",
    "    f1_total += result['f1']\n",
    "\n",
    "# -------- Results --------\n",
    "\n",
    "n = len(gt)\n",
    "print(\"\\nðŸ” Evaluation Summary\")\n",
    "print(\"----------------------\")\n",
    "print(\"True Positives:\", TP_total)\n",
    "print(\"False Positives:\", FP_total)\n",
    "print(\"False Negatives:\", FN_total)\n",
    "print(\"Precision:\", round(precision_total / n, 4))\n",
    "print(\"Recall:   \", round(recall_total / n, 4))\n",
    "print(\"F1 Score: \", round(f1_total / n, 4))\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Count ground-truth attribute frequency\n",
    "attribute_frequency = defaultdict(int)\n",
    "for gt_row in gt:\n",
    "    gt_dict = ast.literal_eval(gt_row)\n",
    "    for key in gt_dict:\n",
    "        attribute_frequency[key] += 1\n",
    "\n",
    "# Step 2: Collect performance per attribute\n",
    "attribute_stats = defaultdict(lambda: {\"TP\": 0, \"FP\": 0, \"FN\": 0})\n",
    "for i in range(len(pred)):\n",
    "    pred_pairs = re.findall(r\"'([^']+)': '([^']+)'\", str(pred[i]))\n",
    "    pred_dict = dict(pred_pairs)\n",
    "    gt_dict = ast.literal_eval(gt[i])\n",
    "\n",
    "    matched_gt_keys = set()\n",
    "    for pred_key, pred_val in pred_dict.items():\n",
    "        matched_gt_key = match_key_fuzzy(pred_key, gt_dict.keys(), threshold=50)\n",
    "        if matched_gt_key and matched_gt_key not in matched_gt_keys:\n",
    "            if fuzzy_match(pred_val, gt_dict[matched_gt_key], threshold=50):\n",
    "                attribute_stats[matched_gt_key][\"TP\"] += 1\n",
    "            else:\n",
    "                attribute_stats[matched_gt_key][\"FP\"] += 1\n",
    "                attribute_stats[matched_gt_key][\"FN\"] += 1\n",
    "            matched_gt_keys.add(matched_gt_key)\n",
    "        else:\n",
    "            attribute_stats[pred_key][\"FP\"] += 1\n",
    "\n",
    "    for gt_key in gt_dict:\n",
    "        if gt_key not in matched_gt_keys:\n",
    "            attribute_stats[gt_key][\"FN\"] += 1\n",
    "\n",
    "# Step 3: Compute F1 per attribute and attach frequency\n",
    "attribute_performance = []\n",
    "for attr, stats in attribute_stats.items():\n",
    "    TP = stats[\"TP\"]\n",
    "    FP = stats[\"FP\"]\n",
    "    FN = stats[\"FN\"]\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    freq = attribute_frequency[attr]\n",
    "    attribute_performance.append((attr, freq, TP, FP, FN, precision, recall, f1))\n",
    "\n",
    "# Step 4: Sort by attribute frequency (high to low)\n",
    "attribute_performance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 5: Print results\n",
    "print(\"\\nðŸ“Š Attribute-level performance (sorted by frequency):\")\n",
    "for attr, freq, TP, FP, FN, precision, recall, f1 in attribute_performance:\n",
    "    print(f\"\\nAttribute: {attr}\")\n",
    "    print(f\"  Frequency: {freq}\")\n",
    "    print(f\"  TP: {TP}, FP: {FP}, FN: {FN}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
